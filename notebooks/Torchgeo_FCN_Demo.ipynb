{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.segmentation import fcn_resnet50\n",
    "import torch.nn as nn\n",
    "from torchgeo.datasets import NAIP, ChesapeakeDE, stack_samples\n",
    "from torchgeo.datasets.utils import download_url\n",
    "from torchgeo.samplers import RandomGeoSampler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minerva.models import FCN8ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://naipeuwest.blob.core.windows.net/naip/v002/de/2018/de_060cm_2018/38075/m_3807511_ne_18_060_20181104.tif to C:\\Users\\hjbak\\AppData\\Local\\Temp\\naip\\m_3807511_ne_18_060_20181104.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 513332284/513332284 [02:22<00:00, 3600595.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://naipeuwest.blob.core.windows.net/naip/v002/de/2018/de_060cm_2018/38075/m_3807511_se_18_060_20181104.tif to C:\\Users\\hjbak\\AppData\\Local\\Temp\\naip\\m_3807511_se_18_060_20181104.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 521985441/521985441 [02:08<00:00, 4069770.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://naipeuwest.blob.core.windows.net/naip/v002/de/2018/de_060cm_2018/38075/m_3807512_nw_18_060_20180815.tif to C:\\Users\\hjbak\\AppData\\Local\\Temp\\naip\\m_3807512_nw_18_060_20180815.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 489865657/489865657 [02:07<00:00, 3853731.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://naipeuwest.blob.core.windows.net/naip/v002/de/2018/de_060cm_2018/38075/m_3807512_sw_18_060_20180815.tif to C:\\Users\\hjbak\\AppData\\Local\\Temp\\naip\\m_3807512_sw_18_060_20180815.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 484476647/484476647 [02:01<00:00, 3992882.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://cicwebresources.blob.core.windows.net/chesapeakebaylandcover/DE/_DE_STATEWIDE.zip to C:\\Users\\hjbak\\AppData\\Local\\Temp\\chesapeake\\_DE_STATEWIDE.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 287350495/287350495 [03:21<00:00, 1426576.23it/s]\n"
     ]
    }
   ],
   "source": [
    "data_root = tempfile.gettempdir()\n",
    "naip_root = os.path.join(data_root, \"naip\")\n",
    "naip_url = \"https://naipeuwest.blob.core.windows.net/naip/v002/de/2018/de_060cm_2018/38075/\"\n",
    "tiles = [\n",
    "    \"m_3807511_ne_18_060_20181104.tif\",\n",
    "    \"m_3807511_se_18_060_20181104.tif\",\n",
    "    \"m_3807512_nw_18_060_20180815.tif\",\n",
    "    \"m_3807512_sw_18_060_20180815.tif\",\n",
    "]\n",
    "for tile in tiles:\n",
    "    download_url(naip_url + tile, naip_root)\n",
    "\n",
    "naip = NAIP(naip_root)\n",
    "\n",
    "chesapeake_root = os.path.join(data_root, \"chesapeake\")\n",
    "\n",
    "chesapeake = ChesapeakeDE(chesapeake_root, crs=naip.crs, res=naip.res, download=True)\n",
    "\n",
    "dataset = naip & chesapeake\n",
    "\n",
    "sampler = RandomGeoSampler(naip, size=300, length=200)\n",
    "dataloader = DataLoader(dataset, sampler=sampler, collate_fn=stack_samples, batch_size=32)\n",
    "\n",
    "testsampler = RandomGeoSampler(naip, size=300, length=8)\n",
    "testdataloader = DataLoader(dataset, sampler=testsampler, collate_fn=stack_samples, batch_size=8, num_workers=4)\n",
    "testdata = list(testdataloader)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m crit \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m      3\u001b[0m \u001b[39m# Criterions are normally parsed to models at init in minerva.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m fcn \u001b[39m=\u001b[39m FCN8ResNet18(crit, input_size\u001b[39m=\u001b[39;49m(\u001b[39m4\u001b[39;49m, \u001b[39m300\u001b[39;49m, \u001b[39m300\u001b[39;49m), n_classes\u001b[39m=\u001b[39;49m\u001b[39m13\u001b[39;49m)\u001b[39m.\u001b[39;49mcuda()\n\u001b[0;32m      5\u001b[0m opt \u001b[39m=\u001b[39m Adam(fcn\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[39m# Optimisers need to be set to a model in minerva before training.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hjbak\\miniconda3\\envs\\minerva-311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:905\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    889\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \n\u001b[0;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[1;32mc:\\Users\\hjbak\\miniconda3\\envs\\minerva-311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hjbak\\miniconda3\\envs\\minerva-311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hjbak\\miniconda3\\envs\\minerva-311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hjbak\\miniconda3\\envs\\minerva-311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mc:\\Users\\hjbak\\miniconda3\\envs\\minerva-311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:905\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    889\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \n\u001b[0;32m    891\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n",
      "File \u001b[1;32mc:\\Users\\hjbak\\miniconda3\\envs\\minerva-311\\Lib\\site-packages\\torch\\cuda\\__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 239\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    242\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "crit = CrossEntropyLoss()\n",
    "\n",
    "# Criterions are normally parsed to models at init in minerva.\n",
    "fcn = FCN8ResNet18(crit, input_size=(4, 300, 300), n_classes=13).cuda()\n",
    "opt = Adam(fcn.parameters(), lr=1e-3)\n",
    "\n",
    "# Optimisers need to be set to a model in minerva before training.\n",
    "fcn.set_optimiser(opt)\n",
    "\n",
    "for epoch in range(101):\n",
    "  losses = []\n",
    "  for i, sample in enumerate(dataloader):\n",
    "    image = sample[\"image\"].cuda().float() / 255.0\n",
    "    target = sample[\"mask\"].cuda().long().squeeze(1)\n",
    "    \n",
    "    # Uses MinervaModel.step.\n",
    "    loss, pred = fcn.step(image, target, train=True)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "  print(epoch, np.mean(losses))\n",
    "  if epoch % 10 == 0:\n",
    "    with torch.no_grad():\n",
    "      image = testdata[\"image\"].cuda().float() / 255.0\n",
    "      target = testdata[\"mask\"].cuda().long().squeeze(1)\n",
    "      pred = fcn(image)\n",
    "\n",
    "      fig, axs = plt.subplots(3, pred.shape[0], figsize=(10,4))\n",
    "      for i in range(pred.shape[0]):\n",
    "        axs[0,i].imshow(image[i].cpu().numpy()[:3].transpose(1,2,0))\n",
    "        axs[1,i].imshow(target[i].cpu().numpy(), cmap=\"Set3\", vmin=0, vmax=12)\n",
    "        axs[2,i].imshow(pred[i].detach().argmax(dim=0).cpu().numpy(), cmap=\"Set3\", vmin=0, vmax=12)\n",
    "      plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn = fcn_resnet50(num_classes=13).cuda()\n",
    "fcn.backbone.conv1 = nn.Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False).cuda()\n",
    "\n",
    "crit = CrossEntropyLoss()\n",
    "opt = Adam(fcn.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(101):\n",
    "  losses = []\n",
    "  for i, sample in enumerate(dataloader):\n",
    "    image = sample[\"image\"].cuda().float() / 255.0\n",
    "    target = sample[\"mask\"].cuda().long().squeeze(1)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    pred = fcn(image)[\"out\"]\n",
    "    loss = crit(pred, target)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "  print(epoch, np.mean(losses))\n",
    "  if epoch % 10 == 0:\n",
    "    with torch.no_grad():\n",
    "      image = testdata[\"image\"].cuda().float() / 255.0\n",
    "      target = testdata[\"mask\"].cuda().long().squeeze(1)\n",
    "      pred = fcn(image)[\"out\"]\n",
    "\n",
    "      fig, axs = plt.subplots(3, pred.shape[0], figsize=(10,4))\n",
    "      for i in range(pred.shape[0]):\n",
    "        axs[0,i].imshow(image[i].cpu().numpy()[:3].transpose(1,2,0))\n",
    "        axs[1,i].imshow(target[i].cpu().numpy(), cmap=\"Set3\", vmin=0, vmax=12)\n",
    "        axs[2,i].imshow(pred[i].detach().argmax(dim=0).cpu().numpy(), cmap=\"Set3\", vmin=0, vmax=12)\n",
    "      plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
    "      plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
